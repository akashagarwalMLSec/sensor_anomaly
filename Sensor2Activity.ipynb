{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    print(lines)\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[s for s in l.split('\\t')] for l in lines]\n",
    "    print(pairs)\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 6\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "['0 1 1 1 1\\t10 10 10 10 10', '1 1 2 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 2 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '2 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 3 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 4 5 6 7\\t10 10 10 10 10', '1 8 1 1 1\\t10 10 10 10 10', '1 9 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '10 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 11 12 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 13 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 14 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 15 15\\t10 10 10 10 11', '16 1 17 15 1\\t5 5 8 17 17', '18 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 9 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 3 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 2 1 2\\t17 17 17 17 17', '19 20 21 2 2\\t17 17 17 17 17', '1 1 1 1 2\\t17 17 17 17 17', '2 2 2 2 2\\t17 17 17 17 17', '1 2 2 22 2\\t17 17 17 17 17', '5 23 24 25 5\\t17 17 17 17 17', '26 27 28 1 1\\t17 17 17 17 17', '1 1 1 1 22\\t17 17 17 17 17', '1 1 1 1 22\\t17 17 17 17 17', '2 2 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 2 2\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '29 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 9 1\\t17 17 17 17 17', '1 1 1 9 1\\t17 17 17 17 17', '1 1 1 9 1\\t17 17 17 17 17', '1 2 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 3 1\\t17 17 17 17 17', '29 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 1 1\\t17 17 17 17 17', '1 1 1 29 30\\t17 17 17 17 10', '31 32 33 34 35\\t15 15 3 3 18', '36 37 38 39 40\\t13 13 13 13 13', '41 42 43 44 45\\t13 13 13 13 13', '46 47 48 37 1\\t13 13 9 1 1', '1 49 50 30 30\\t1 1 1 1 1', '51 52 53 54 55\\t1 1 1 1 1', '56 57 37 58 59\\t1 1 1 19 12', '60 61 62 63 64\\t12 23 0 0 0', '65 62 1 63 63\\t0 0 0 0 0', '63 1 1 63 1\\t0 0 0 0 0', '1 63 63 63 1\\t0 0 0 0 0', '63 63 1 63 1\\t0 0 0 0 0', '63 63 63 63 63\\t0 0 0 0 0', '1 63 1 63 63\\t0 0 0 0 0', '1 63 1 63 1\\t0 0 0 0 0', '63 63 1 63 62\\t0 0 0 0 0', '66 67 68 69 68\\t7 1 1 19 10', '70 65 62 62 63\\t10 23 0 0 0', '63 1 1 1 1\\t0 0 0 0 0', '71 70 72 63 63\\t0 0 0 0 0', '73 62 63 63 63\\t0 0 0 0 0', '63 63 63 63 63\\t0 0 0 0 0', '1 1 1 1 1\\t0 0 0 0 0', '63 63 74 75 76\\t0 0 23 10 10', '77 77 78 78 79\\t10 10 10 10 10', '80 81 80 68 80\\t10 10 10 10 19', '82 81 1 1 1\\t1 1 1 1 1', '1 83 84 84 68\\t1 1 1 1 1', '85 86 87 88 1\\t21 14 20 10 10', '88 78 89 90 1\\t10 10 10 10 10', '1 1 91 92 93\\t10 10 10 2 22', '94 1 95 94 94\\t22 22 22 22 22', '94 94 96 96 97\\t22 22 22 22 22', '94 94 22 1 1\\t22 2 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 1 1\\t10 10 10 10 10', '1 1 1 22 1\\t10 10 10 10 10', '18 22 2 2 2\\t10 10 10 10 10', '2 2 2 2 2\\t10 10 10 10 10', '2 1 1 1 1\\t10 10 10 10 10', '1 1 2 2 29\\t10 10 10 10 10', '22 51 98 10 1\\t10 10 10 10 10', '99 1 54 54 100\\t10 10 10 10 10', '101 102 1 1 1\\t10 10 10 10 10', '103 93 104 105 106\\t10 10 10 10 10', '107 108 109 110 111\\t10 10 10 10 10', '1 112 1 1 1\\t10 12 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '3 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 1 1 1\\t6 6 6 6 6', '1 1 113 114 115\\t6 6 4 13 13', '116 117 118 119 120\\t13 13 13 16 13', '121 122 1 117 123\\t13 13 13 13 13', '124 117 56 56 56\\t13 13 13 13 13', '125 56 117 117 117\\t13 13 13 13 13', '124 1 56 56 56\\t13 13 13 13 13', '124 56 56 117 117\\t13 13 13 13 13', '125 117 117 56 1\\t13 13 13 13 13', '56 56 56 56 117\\t13 13 13 13 13', '56 126 1 56 56\\t13 13 13 13 13', '56 126 56 56 117\\t13 13 13 13 13', '117 127 96 95 96\\t13 13 13 13 13', '128 129 70 130 131\\t13 13 13 13 13', '132 70 133 131 131\\t13 13 13 13 13']\n",
      "[['0 1 1 1 1', '10 10 10 10 10'], ['1 1 2 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 2 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['2 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 3 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 4 5 6 7', '10 10 10 10 10'], ['1 8 1 1 1', '10 10 10 10 10'], ['1 9 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['10 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 11 12 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 13 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 14 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 15 15', '10 10 10 10 11'], ['16 1 17 15 1', '5 5 8 17 17'], ['18 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 9 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 3 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 2 1 2', '17 17 17 17 17'], ['19 20 21 2 2', '17 17 17 17 17'], ['1 1 1 1 2', '17 17 17 17 17'], ['2 2 2 2 2', '17 17 17 17 17'], ['1 2 2 22 2', '17 17 17 17 17'], ['5 23 24 25 5', '17 17 17 17 17'], ['26 27 28 1 1', '17 17 17 17 17'], ['1 1 1 1 22', '17 17 17 17 17'], ['1 1 1 1 22', '17 17 17 17 17'], ['2 2 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 2 2', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['29 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 9 1', '17 17 17 17 17'], ['1 1 1 9 1', '17 17 17 17 17'], ['1 1 1 9 1', '17 17 17 17 17'], ['1 2 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 3 1', '17 17 17 17 17'], ['29 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 1 1', '17 17 17 17 17'], ['1 1 1 29 30', '17 17 17 17 10'], ['31 32 33 34 35', '15 15 3 3 18'], ['36 37 38 39 40', '13 13 13 13 13'], ['41 42 43 44 45', '13 13 13 13 13'], ['46 47 48 37 1', '13 13 9 1 1'], ['1 49 50 30 30', '1 1 1 1 1'], ['51 52 53 54 55', '1 1 1 1 1'], ['56 57 37 58 59', '1 1 1 19 12'], ['60 61 62 63 64', '12 23 0 0 0'], ['65 62 1 63 63', '0 0 0 0 0'], ['63 1 1 63 1', '0 0 0 0 0'], ['1 63 63 63 1', '0 0 0 0 0'], ['63 63 1 63 1', '0 0 0 0 0'], ['63 63 63 63 63', '0 0 0 0 0'], ['1 63 1 63 63', '0 0 0 0 0'], ['1 63 1 63 1', '0 0 0 0 0'], ['63 63 1 63 62', '0 0 0 0 0'], ['66 67 68 69 68', '7 1 1 19 10'], ['70 65 62 62 63', '10 23 0 0 0'], ['63 1 1 1 1', '0 0 0 0 0'], ['71 70 72 63 63', '0 0 0 0 0'], ['73 62 63 63 63', '0 0 0 0 0'], ['63 63 63 63 63', '0 0 0 0 0'], ['1 1 1 1 1', '0 0 0 0 0'], ['63 63 74 75 76', '0 0 23 10 10'], ['77 77 78 78 79', '10 10 10 10 10'], ['80 81 80 68 80', '10 10 10 10 19'], ['82 81 1 1 1', '1 1 1 1 1'], ['1 83 84 84 68', '1 1 1 1 1'], ['85 86 87 88 1', '21 14 20 10 10'], ['88 78 89 90 1', '10 10 10 10 10'], ['1 1 91 92 93', '10 10 10 2 22'], ['94 1 95 94 94', '22 22 22 22 22'], ['94 94 96 96 97', '22 22 22 22 22'], ['94 94 22 1 1', '22 2 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 1 1', '10 10 10 10 10'], ['1 1 1 22 1', '10 10 10 10 10'], ['18 22 2 2 2', '10 10 10 10 10'], ['2 2 2 2 2', '10 10 10 10 10'], ['2 1 1 1 1', '10 10 10 10 10'], ['1 1 2 2 29', '10 10 10 10 10'], ['22 51 98 10 1', '10 10 10 10 10'], ['99 1 54 54 100', '10 10 10 10 10'], ['101 102 1 1 1', '10 10 10 10 10'], ['103 93 104 105 106', '10 10 10 10 10'], ['107 108 109 110 111', '10 10 10 10 10'], ['1 112 1 1 1', '10 12 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['3 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 1 1 1', '6 6 6 6 6'], ['1 1 113 114 115', '6 6 4 13 13'], ['116 117 118 119 120', '13 13 13 16 13'], ['121 122 1 117 123', '13 13 13 13 13'], ['124 117 56 56 56', '13 13 13 13 13'], ['125 56 117 117 117', '13 13 13 13 13'], ['124 1 56 56 56', '13 13 13 13 13'], ['124 56 56 117 117', '13 13 13 13 13'], ['125 117 117 56 1', '13 13 13 13 13'], ['56 56 56 56 117', '13 13 13 13 13'], ['56 126 1 56 56', '13 13 13 13 13'], ['56 126 56 56 117', '13 13 13 13 13'], ['117 127 96 95 96', '13 13 13 13 13'], ['128 129 70 130 131', '13 13 13 13 13'], ['132 70 133 131 131', '13 13 13 13 13']]\n",
      "Read 190 sentence pairs\n",
      "Trimmed to 190 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "Activity 26\n",
      "Sensor 136\n",
      "['13 13 13 13 13', '121 122 1 117 123']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('Sensor', 'Activity', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7ea4b06f8a39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-774ce8edefa6>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 18\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-b2baf8fbdbad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_words, attentions = evaluate(encoder1, attn_decoder1, \"\")\n",
    "# plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
    "\n",
    "evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
